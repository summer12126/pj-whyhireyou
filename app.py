import streamlit as st
import os
from dotenv import load_dotenv
from langchain.vectorstores.elasticsearch import ElasticsearchStore
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° í™˜ê²½ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL")
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
INDEX_NAME = os.getenv("INDEX_NAME", "documents")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RAG ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("RAG ì±—ë´‡ ì„¤ì •")
model_name = st.sidebar.selectbox("ëª¨ë¸ ì„ íƒ", ["gpt-3.5-turbo", "gpt-4"])
# temperature = st.sidebar.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.1)

# ë©”ì¸ í˜ì´ì§€ ì„¤ì •
st.title("RAG ê¸°ë°˜ ì±—ë´‡")
st.markdown("Elasticsearchì™€ LangChainì„ í™œìš©í•œ RAG ê¸°ë°˜ ì±—ë´‡ì…ë‹ˆë‹¤.")



# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "conversation" not in st.session_state:
    # Elasticsearch ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    
    # Elasticsearch ì—°ê²° ì„¤ì •
    vector_store = ElasticsearchStore(
        es_url=ELASTICSEARCH_URL,
        index_name=INDEX_NAME,
        embedding=embeddings,
        es_user=ELASTICSEARCH_USERNAME,
        es_password=ELASTICSEARCH_PASSWORD
    )
    
    # ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model_name=model_name,
        # temperature=temperature,
        openai_api_key=OPENAI_API_KEY
    )
    
    # ëŒ€í™” ì²´ì¸ ìƒì„±
    st.session_state.conversation = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(),
        memory=memory,
        return_source_documents=True
    )

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("ìƒê° ì¤‘..."):
            response = st.session_state.conversation({"question": prompt})
            answer = response["answer"]
            
            # ì¶œì²˜ ì •ë³´ ì¶”ê°€
            if "source_documents" in response and response["source_documents"]:
                sources = set()
                for doc in response["source_documents"]:
                    if hasattr(doc, "metadata") and "source" in doc.metadata:
                        sources.add(doc.metadata["source"])
                
                if sources:
                    answer += "\n\n**ì¶œì²˜:**\n"
                    for i, source in enumerate(sources, 1):
                        answer += f"{i}. {source}\n"
    
        message_placeholder.markdown(answer)
    
    # ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": answer})
